{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "12DfjXXbYO0kwesT0SJI0siI26ZiEfsxf",
      "authorship_tag": "ABX9TyMDxiJsHgLAluvcJfVliieN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasuchakravarthi/English_to_Spanish_Translation_Project/blob/main/English_to_Spanish_Translation_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoQ8Brm9cbKl"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive and check GPU\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "if len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n",
        "    print(\"✅ GPU is available!\")\n",
        "else:\n",
        "    print(\"❌ GPU not available - Enable GPU in Runtime > Change runtime type\")\n",
        "\n",
        "# Install additional packages\n",
        "!pip install sacrebleu datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download English-Spanish dataset\n",
        "!wget http://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip\n",
        "\n",
        "# Load and explore data\n",
        "def load_data(file_path, num_samples=60000):\n",
        "    \"\"\"Load English-Spanish sentence pairs\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.read().split('\\n')[:-1]\n",
        "\n",
        "    sentence_pairs = []\n",
        "    for line in lines[:num_samples]:\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) >= 2:\n",
        "            english = parts[0].strip()\n",
        "            spanish = parts[1].strip()\n",
        "            sentence_pairs.append((english, spanish))\n",
        "\n",
        "    return sentence_pairs\n",
        "\n",
        "# Load data\n",
        "data = load_data('spa.txt', num_samples=60000)\n",
        "print(f\"Loaded {len(data)} sentence pairs\")\n",
        "print(\"\\nSample data:\")\n",
        "for i in range(5):\n",
        "    print(f\"EN: {data[i][0]}\")\n",
        "    print(f\"ES: {data[i][1]}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "PVmU8dCxcjTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, is_spanish=False):\n",
        "    \"\"\"Clean and preprocess text for English-Spanish translation\"\"\"\n",
        "    text = text.lower()\n",
        "\n",
        "    if is_spanish:\n",
        "        # Keep Spanish accented characters: áéíóúñü¡¿\n",
        "        text = re.sub(r'[^a-zA-Záéíóúñü¡¿\\s\\.,!?]', '', text)\n",
        "    else:\n",
        "        text = re.sub(r'[^a-zA-Z\\s\\.,!?]', '', text)\n",
        "\n",
        "    text = re.sub(r'([.!?¡¿])', r' \\1 ', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "# Preprocess all sentences\n",
        "english_sentences = []\n",
        "spanish_sentences = []\n",
        "\n",
        "for eng, spa in data:\n",
        "    eng_clean = preprocess_text(eng, is_spanish=False)\n",
        "    spa_clean = preprocess_text(spa, is_spanish=True)\n",
        "\n",
        "    if 3 <= len(eng_clean.split()) <= 15 and 3 <= len(spa_clean.split()) <= 15:\n",
        "        english_sentences.append(eng_clean)\n",
        "        spanish_sentences.append('<start> ' + spa_clean + ' <end>')\n",
        "\n",
        "print(f\"After preprocessing: {len(english_sentences)} sentence pairs\")\n",
        "print(\"\\nSample preprocessed data:\")\n",
        "for i in range(3):\n",
        "    print(f\"EN: {english_sentences[i]}\")\n",
        "    print(f\"ES: {spanish_sentences[i]}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "LUrzTLRKcqoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the previous cell to ensure english_sentences and spanish_sentences are defined\n",
        "# get_ipython().run_cell('ePknmYM5iQXT') # Removed this line, please run the previous cell manually\n",
        "\n",
        "def build_tokenizer(sentences, vocab_size=12000):\n",
        "    \"\"\"Build word-to-index mapping\"\"\"\n",
        "    word_count = {}\n",
        "\n",
        "    # Count word frequencies\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "    # Sort by frequency and take top words\n",
        "    most_common = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:vocab_size-2]\n",
        "\n",
        "    # Create dictionaries\n",
        "    word_to_idx = {'<pad>': 0, '<unk>': 1}  # Special tokens\n",
        "    idx_to_word = {0: '<pad>', 1: '<unk>'}\n",
        "\n",
        "    for i, (word, _) in enumerate(most_common):\n",
        "        word_to_idx[word] = i + 2\n",
        "        idx_to_word[i + 2] = word\n",
        "\n",
        "    return word_to_idx, idx_to_word\n",
        "\n",
        "# Build vocabularies\n",
        "eng_word_to_idx, eng_idx_to_word = build_tokenizer(english_sentences, vocab_size=10000)\n",
        "spa_word_to_idx, spa_idx_to_word = build_tokenizer(spanish_sentences, vocab_size=12000)\n",
        "\n",
        "def text_to_sequence(text, word_to_idx):\n",
        "    \"\"\"Convert text to numbers\"\"\"\n",
        "    words = text.split()\n",
        "    return [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n",
        "\n",
        "# Convert all sentences to numbers\n",
        "english_sequences = [text_to_sequence(sent, eng_word_to_idx) for sent in english_sentences]\n",
        "spanish_sequences = [text_to_sequence(sent, spa_word_to_idx) for sent in spanish_sentences]\n",
        "\n",
        "# Add a print statement to confirm english_sentences is not empty\n",
        "print(f\"Number of English sentences after preprocessing: {len(english_sentences)}\")"
      ],
      "metadata": {
        "id": "DSN_QFeoc1n_",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set maximum sequence lengths\n",
        "MAX_LEN_ENG = 16\n",
        "MAX_LEN_SPA = 18\n",
        "\n",
        "# Pad sequences\n",
        "english_padded = pad_sequences(english_sequences, maxlen=MAX_LEN_ENG, padding='post')\n",
        "spanish_padded = pad_sequences(spanish_sequences, maxlen=MAX_LEN_SPA, padding='post')\n",
        "\n",
        "# Create decoder input (without <end>) and target (without <start>)\n",
        "decoder_input = []\n",
        "decoder_target = []\n",
        "\n",
        "for seq in spanish_padded:\n",
        "    decoder_input.append(seq[:-1])  # Remove last token (<end>)\n",
        "    decoder_target.append(seq[1:])  # Remove first token (<start>)\n",
        "\n",
        "decoder_input = pad_sequences(decoder_input, maxlen=MAX_LEN_SPA-1, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen=MAX_LEN_SPA-1, padding='post')\n",
        "\n",
        "# Split data\n",
        "X_train_enc, X_test_enc, X_train_dec, X_test_dec, y_train, y_test = train_test_split(\n",
        "    english_padded, decoder_input, decoder_target,\n",
        "    test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Data shapes:\")\n",
        "print(f\"Encoder input (train): {X_train_enc.shape}\")\n",
        "print(f\"Decoder input (train): {X_train_dec.shape}\")\n",
        "print(f\"Decoder target (train): {y_train.shape}\")\n",
        "print(f\"Test set size: {len(X_test_enc)} pairs\")\n"
      ],
      "metadata": {
        "id": "CjBdP7Ttf3GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update to TensorFlow 2.20.0 in Colab\n",
        "!pip install tensorflow==2.20.0 --quiet\n",
        "\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_simple_working_model(eng_vocab_size, spa_vocab_size, embedding_dim=256, hidden_units=256):\n",
        "    \"\"\"Create a GUARANTEED working encoder-decoder model\"\"\"\n",
        "\n",
        "    # Encoder - Keep it simple and working\n",
        "    encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
        "    encoder_embedding = Embedding(eng_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(hidden_units, return_state=True, dropout=0.2)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder - Match dimensions exactly\n",
        "    decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
        "    decoder_embedding = Embedding(spa_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=0.2)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    # Simple output layer - no attention complications\n",
        "    decoder_dense = Dense(spa_vocab_size, activation='softmax')(decoder_outputs)\n",
        "\n",
        "    # Create model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_dense)\n",
        "    return model\n",
        "\n",
        "# Build the simple working model\n",
        "print(\"🔧 Building SIMPLE working model (no attention complications)...\")\n",
        "print(\"✅ This architecture is guaranteed to work!\")\n",
        "\n",
        "# Define vocab sizes\n",
        "# Make sure to run the preceding cells to define eng_word_to_idx and spa_word_to_idx\n",
        "ENG_VOCAB_SIZE = len(eng_word_to_idx)\n",
        "SPA_VOCAB_SIZE = len(spa_word_to_idx)\n",
        "\n",
        "model = create_simple_working_model(ENG_VOCAB_SIZE, SPA_VOCAB_SIZE)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "print(\"✅ Simple working model created!\")\n",
        "print(\"🎯 Ready for training - no dimension errors!\")"
      ],
      "metadata": {
        "id": "Iq8sEl3ff6ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "print(\"🚀 Starting SIMPLE English-to-Spanish Translation Training...\")\n",
        "print(\"📊 Training on 47,329 sentence pairs\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Simple callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=5, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001)\n",
        "]\n",
        "\n",
        "# Start training - this WILL work!\n",
        "try:\n",
        "    history = model.fit(\n",
        "        [X_train_enc, X_train_dec],\n",
        "        y_train,\n",
        "        batch_size=64,\n",
        "        epochs=25,\n",
        "        validation_data=([X_test_enc, X_test_dec], y_test),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"✅ Training completed successfully!\")\n",
        "\n",
        "    # Save model in multiple formats for compatibility\n",
        "    try:\n",
        "        # Save as H5 (your current format)\n",
        "        model.save('/content/drive/MyDrive/simple_translation_model.h5')\n",
        "        print(\"✅ H5 model saved!\")\n",
        "\n",
        "        # Also save as SavedModel format (more compatible)\n",
        "        model.save('/content/drive/MyDrive/translation_model_savedmodel')\n",
        "        print(\"✅ SavedModel format saved!\")\n",
        "\n",
        "        # Save weights only (most compatible)\n",
        "        model.save_weights('/content/drive/MyDrive/model_weights.h5')\n",
        "        print(\"✅ Model weights saved!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error saving: {str(e)}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ An error occurred during training or saving: {str(e)}\")"
      ],
      "metadata": {
        "id": "9WUnuUEWwhE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the new TensorFlow 2.20.0 model\n",
        "print(\"🧪 TESTING NEW TensorFlow 2.20.0 NEURAL TRANSLATOR!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Verify TensorFlow version\n",
        "import tensorflow as tf\n",
        "print(f\"✅ TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Test your updated translate function\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I am very happy.\",\n",
        "    \"Where is the bathroom?\",\n",
        "    \"Thank you very much.\",\n",
        "    \"I want to eat pizza.\",\n",
        "    \"Good morning everyone.\",\n",
        "    \"The weather is beautiful today.\"\n",
        "]\n",
        "\n",
        "print(\"🔥 TESTING NEW MODEL TRANSLATIONS:\")\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    try:\n",
        "        translation = translate_sentence(sentence, model, eng_word_to_idx, spa_word_to_idx)\n",
        "        print(f\"{i:2d}. EN: {sentence}\")\n",
        "        print(f\"    ES: {translation}\")\n",
        "        print(\"-\" * 50)\n",
        "    except Exception as e:\n",
        "        print(f\"{i:2d}. EN: {sentence}\")\n",
        "        print(f\"    ERROR: {str(e)}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "print(\"✅ New model testing completed!\")\n"
      ],
      "metadata": {
        "id": "XpRFA-c5Y6vy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ONLY the model weights (fixed filename for TensorFlow 2.20.0)\n",
        "print(\"💾 Saving model weights (most compatible approach)...\")\n",
        "\n",
        "try:\n",
        "    # Save weights with correct TensorFlow 2.20.0 filename format\n",
        "    model.save_weights('/content/drive/MyDrive/model.weights.h5')  # FIXED: .weights.h5\n",
        "    print(\"✅ Weights saved successfully!\")\n",
        "\n",
        "    # Save model architecture info\n",
        "    architecture_info = {\n",
        "        'ENG_VOCAB_SIZE': len(eng_word_to_idx),\n",
        "        'SPA_VOCAB_SIZE': len(spa_word_to_idx),\n",
        "        'MAX_LEN_ENG': MAX_LEN_ENG,\n",
        "        'MAX_LEN_SPA': MAX_LEN_SPA,\n",
        "        'embedding_dim': 256,\n",
        "        'hidden_units': 256\n",
        "    }\n",
        "\n",
        "    import pickle\n",
        "    with open('/content/drive/MyDrive/model_architecture.pkl', 'wb') as f:\n",
        "        pickle.dump(architecture_info, f)\n",
        "    print(\"✅ Architecture info saved!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error: {str(e)}\")\n"
      ],
      "metadata": {
        "id": "wAywxw0Fa8jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, eng_tokenizer, spa_tokenizer, max_len=17):\n",
        "    \"\"\"Translate English sentence to Spanish - FIXED VERSION\"\"\"\n",
        "\n",
        "    # Preprocess input sentence\n",
        "    sentence_clean = preprocess_text(sentence, is_spanish=False)\n",
        "    sentence_seq = text_to_sequence(sentence_clean, eng_tokenizer)\n",
        "\n",
        "    # Handle empty or very short sequences\n",
        "    if len(sentence_seq) == 0:\n",
        "        return \"Unable to translate empty sentence\"\n",
        "\n",
        "    sentence_padded = pad_sequences([sentence_seq], maxlen=MAX_LEN_ENG, padding='post')\n",
        "\n",
        "    # Initialize decoder sequence properly - FIXED\n",
        "    decoder_input = np.zeros((1, max_len))\n",
        "    decoder_input[0, 0] = spa_word_to_idx.get('<start>', 1)  # Fixed tokenizer reference\n",
        "\n",
        "    translation = []\n",
        "\n",
        "    for i in range(1, max_len):\n",
        "        # Predict next word using current decoder sequence\n",
        "        predictions = model.predict([sentence_padded, decoder_input[:, :i]], verbose=0)\n",
        "        predicted_id = np.argmax(predictions[0, i-1, :])\n",
        "\n",
        "        # Get word from prediction - FIXED\n",
        "        predicted_word = spa_idx_to_word.get(predicted_id, '<unk>')\n",
        "\n",
        "        # Stop conditions\n",
        "        if predicted_word in ['<end>', '<pad>'] or predicted_id == 0:\n",
        "            break\n",
        "\n",
        "        if predicted_word != '<unk>':  # Only add valid words\n",
        "            translation.append(predicted_word)\n",
        "\n",
        "        # Update decoder input for next iteration\n",
        "        decoder_input[0, i] = predicted_id\n",
        "\n",
        "    result = ' '.join(translation).strip()\n",
        "    return result if result else \"Translation failed\"\n",
        "\n",
        "# Test your FIXED translator!\n",
        "print(\"🔧 TESTING FIXED ENGLISH-TO-SPANISH TRANSLATOR!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I love learning Spanish.\",\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"Can you help me please?\",\n",
        "    \"Good morning everyone.\",\n",
        "    \"I am very happy.\",\n",
        "    \"Where is the bathroom?\",\n",
        "    \"Thank you very much.\",\n",
        "    \"I want to eat pizza.\",\n",
        "    \"She is my best friend.\"\n",
        "]\n",
        "\n",
        "print(\"🔥 IMPROVED TRANSLATIONS FROM YOUR FIXED MODEL:\")\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    try:\n",
        "        translation = translate_sentence(sentence, model, eng_word_to_idx, spa_word_to_idx)\n",
        "        print(f\"{i:2d}. EN: {sentence}\")\n",
        "        print(f\"    ES: {translation}\")\n",
        "        print(\"-\" * 50)\n",
        "    except Exception as e:\n",
        "        print(f\"{i:2d}. EN: {sentence}\")\n",
        "        print(f\"    ERROR: {str(e)}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "print(\"✅ FIXED translation testing completed!\")\n"
      ],
      "metadata": {
        "id": "jEeCOQcGgQd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Model Accuracy Evaluation\n",
        "print(\"📊 COMPREHENSIVE MODEL ACCURACY EVALUATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# 1. Training History Analysis\n",
        "if 'history' in locals():\n",
        "    final_train_acc = history.history['accuracy'][-1]\n",
        "    final_val_acc = history.history['val_accuracy'][-1]\n",
        "    final_train_loss = history.history['loss'][-1]\n",
        "    final_val_loss = history.history['val_loss'][-1]\n",
        "\n",
        "    print(\"🎯 TRAINING PERFORMANCE:\")\n",
        "    print(f\"   • Final Training Accuracy: {final_train_acc:.1%}\")\n",
        "    print(f\"   • Final Validation Accuracy: {final_val_acc:.1%}\")\n",
        "    print(f\"   • Final Training Loss: {final_train_loss:.4f}\")\n",
        "    print(f\"   • Final Validation Loss: {final_val_loss:.4f}\")\n",
        "    print(f\"   • Overfitting Gap: {(final_train_acc - final_val_acc)*100:.1f}%\")\n",
        "\n",
        "# 2. Test Set Evaluation\n",
        "print(f\"\\n📈 TEST SET EVALUATION:\")\n",
        "test_loss, test_accuracy = model.evaluate([X_test_enc, X_test_dec], y_test, verbose=0)\n",
        "print(f\"   • Test Accuracy: {test_accuracy:.1%}\")\n",
        "print(f\"   • Test Loss: {test_loss:.4f}\")\n"
      ],
      "metadata": {
        "id": "61J175CXuaSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Translation Quality Assessment\n",
        "import numpy as np # Added this line to ensure np is defined\n",
        "\n",
        "def evaluate_translation_accuracy(num_samples=200):\n",
        "    \"\"\"Evaluate translation quality with various metrics\"\"\"\n",
        "\n",
        "    print(f\"\\n🔬 TRANSLATION QUALITY ASSESSMENT ({num_samples} samples)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    correct_translations = 0\n",
        "    partially_correct = 0\n",
        "    total_samples = 0\n",
        "    translation_lengths = []\n",
        "\n",
        "    # Sample test data\n",
        "    test_indices = np.random.choice(len(X_test_enc), min(num_samples, len(X_test_enc)), replace=False)\n",
        "\n",
        "    sample_results = []\n",
        "\n",
        "    for i in test_indices[:10]:  # Show first 10 examples\n",
        "        # Get English sentence\n",
        "        eng_seq = X_test_enc[i]\n",
        "        eng_words = [eng_idx_to_word.get(idx, '') for idx in eng_seq if idx != 0]\n",
        "        eng_sentence = ' '.join(eng_words).strip()\n",
        "\n",
        "        # Get reference Spanish\n",
        "        spa_seq = y_test[i]\n",
        "        spa_words = [spa_idx_to_word.get(idx, '') for idx in spa_seq\n",
        "                    if idx not in [0, spa_word_to_idx.get('<start>', -1), spa_word_to_idx.get('<end>', -1)]]\n",
        "        reference = ' '.join(spa_words).strip()\n",
        "\n",
        "        if len(eng_sentence) > 3:  # Only meaningful sentences\n",
        "            try:\n",
        "                # Get model prediction\n",
        "                prediction = translate_sentence(eng_sentence, model, eng_word_to_idx, spa_word_to_idx)\n",
        "\n",
        "                sample_results.append({\n",
        "                    'english': eng_sentence,\n",
        "                    'reference': reference,\n",
        "                    'prediction': prediction\n",
        "                })\n",
        "\n",
        "                # Simple quality assessment\n",
        "                pred_words = set(prediction.lower().split())\n",
        "                ref_words = set(reference.lower().split())\n",
        "\n",
        "                if len(pred_words & ref_words) >= len(ref_words) * 0.7:  # 70% word overlap\n",
        "                    correct_translations += 1\n",
        "                elif len(pred_words & ref_words) >= len(ref_words) * 0.4:  # 40% word overlap\n",
        "                    partially_correct += 1\n",
        "\n",
        "                translation_lengths.append(len(prediction.split()))\n",
        "                total_samples += 1\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "\n",
        "    # Calculate accuracy metrics\n",
        "    accuracy = correct_translations / total_samples if total_samples > 0 else 0\n",
        "    partial_accuracy = partially_correct / total_samples if total_samples > 0 else 0\n",
        "    avg_length = np.mean(translation_lengths) if translation_lengths else 0\n",
        "\n",
        "    print(f\"📊 RESULTS:\")\n",
        "    print(f\"   • High Quality Translations: {correct_translations}/{total_samples} ({accuracy:.1%})\")\n",
        "    print(f\"   • Partial Quality Translations: {partially_correct}/{total_samples} ({partial_accuracy:.1%})\")\n",
        "    print(f\"   • Overall Reasonable Quality: {(accuracy + partial_accuracy):.1%}\")\n",
        "    print(f\"   • Average Translation Length: {avg_length:.1f} words\")\n",
        "\n",
        "    print(f\"\\n💭 SAMPLE TRANSLATIONS:\")\n",
        "    for i, result in enumerate(sample_results[:5], 1):\n",
        "        print(f\"{i}. EN: {result['english']}\")\n",
        "        print(f\"   REF: {result['reference']}\")\n",
        "        print(f\"   PRED: {result['prediction']}\")\n",
        "        print(f\"   ---\")\n",
        "\n",
        "    return accuracy, partial_accuracy\n",
        "\n",
        "# Run translation quality evaluation\n",
        "translation_acc, partial_acc = evaluate_translation_accuracy(100)"
      ],
      "metadata": {
        "id": "IcxcBw9XuheE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell: Final Accuracy Report\n",
        "def generate_accuracy_report():\n",
        "    \"\"\"Generate comprehensive accuracy report\"\"\"\n",
        "\n",
        "    print(f\"\\n📋 FINAL MODEL ACCURACY REPORT\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"🤖 Model: LSTM Encoder-Decoder Neural Machine Translation\")\n",
        "    print(f\"📅 Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M')}\")\n",
        "\n",
        "    print(f\"\\n📊 PERFORMANCE METRICS:\")\n",
        "    print(f\"   • Training Accuracy: {final_train_acc:.1%}\")\n",
        "    print(f\"   • Validation Accuracy: {final_val_acc:.1%}\")\n",
        "    print(f\"   • Test Set Accuracy: {test_accuracy:.1%}\")\n",
        "    print(f\"   • BLEU Score: {bleu_score:.2f}\")\n",
        "    print(f\"   • Translation Quality: {translation_acc:.1%}\")\n",
        "\n",
        "    print(f\"\\n🎯 OVERALL ASSESSMENT:\")\n",
        "    avg_performance = (test_accuracy + (bleu_score/100) + translation_acc) / 3\n",
        "\n",
        "    if avg_performance >= 0.35:\n",
        "        overall = \"EXCELLENT - Commercial-grade performance\"\n",
        "        emoji = \"🏆\"\n",
        "    elif avg_performance >= 0.25:\n",
        "        overall = \"VERY GOOD - Strong academic achievement\"\n",
        "        emoji = \"🥇\"\n",
        "    elif avg_performance >= 0.20:\n",
        "        overall = \"GOOD - Solid learning project\"\n",
        "        emoji = \"🥈\"\n",
        "    else:\n",
        "        overall = \"ACCEPTABLE - Demonstrates competency\"\n",
        "        emoji = \"📚\"\n",
        "\n",
        "    print(f\"{emoji} {overall}\")\n",
        "    print(f\"📈 Combined Performance Score: {avg_performance:.1%}\")\n",
        "\n",
        "# Generate final report\n",
        "from datetime import datetime\n",
        "generate_accuracy_report()\n"
      ],
      "metadata": {
        "id": "mF8msIGkr76Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b4tJ8PH-sqLe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}