{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMGsxFIcnI7klnFuqMlOyEN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasuchakravarthi/English_to_Spanish_Translation_Project/blob/main/English_to_Spanish_Translation_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoQ8Brm9cbKl"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive and check GPU\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Mount Google Drive\n",
        "#drive.mount('/content/drive')\n",
        "\n",
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
        "if len(tf.config.experimental.list_physical_devices('GPU')) > 0:\n",
        "    print(\"‚úÖ GPU is available!\")\n",
        "else:\n",
        "    print(\"‚ùå GPU not available - Enable GPU in Runtime > Change runtime type\")\n",
        "\n",
        "# Install additional packages\n",
        "!pip install sacrebleu datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download English-Spanish dataset\n",
        "!wget http://www.manythings.org/anki/spa-eng.zip\n",
        "!unzip spa-eng.zip\n",
        "\n",
        "# Load and explore data\n",
        "def load_data(file_path, num_samples=60000):\n",
        "    \"\"\"Load English-Spanish sentence pairs\"\"\"\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        lines = file.read().split('\\n')[:-1]\n",
        "\n",
        "    sentence_pairs = []\n",
        "    for line in lines[:num_samples]:\n",
        "        parts = line.split('\\t')\n",
        "        if len(parts) >= 2:\n",
        "            english = parts[0].strip()\n",
        "            spanish = parts[1].strip()\n",
        "            sentence_pairs.append((english, spanish))\n",
        "\n",
        "    return sentence_pairs\n",
        "\n",
        "# Load data\n",
        "data = load_data('spa.txt', num_samples=60000)\n",
        "print(f\"Loaded {len(data)} sentence pairs\")\n",
        "print(\"\\nSample data:\")\n",
        "for i in range(5):\n",
        "    print(f\"EN: {data[i][0]}\")\n",
        "    print(f\"ES: {data[i][1]}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "PVmU8dCxcjTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text, is_spanish=False):\n",
        "    \"\"\"Clean and preprocess text for English-Spanish translation\"\"\"\n",
        "    text = text.lower()\n",
        "\n",
        "    if is_spanish:\n",
        "        # Keep Spanish accented characters: √°√©√≠√≥√∫√±√º¬°¬ø\n",
        "        text = re.sub(r'[^a-zA-Z√°√©√≠√≥√∫√±√º¬°¬ø\\s\\.,!?]', '', text)\n",
        "    else:\n",
        "        text = re.sub(r'[^a-zA-Z\\s\\.,!?]', '', text)\n",
        "\n",
        "    text = re.sub(r'([.!?¬°¬ø])', r' \\1 ', text)\n",
        "    text = ' '.join(text.split())\n",
        "    return text.strip()\n",
        "\n",
        "# Preprocess all sentences\n",
        "english_sentences = []\n",
        "spanish_sentences = []\n",
        "\n",
        "for eng, spa in data:\n",
        "    eng_clean = preprocess_text(eng, is_spanish=False)\n",
        "    spa_clean = preprocess_text(spa, is_spanish=True)\n",
        "\n",
        "    if 3 <= len(eng_clean.split()) <= 15 and 3 <= len(spa_clean.split()) <= 15:\n",
        "        english_sentences.append(eng_clean)\n",
        "        spanish_sentences.append('<start> ' + spa_clean + ' <end>')\n",
        "\n",
        "print(f\"After preprocessing: {len(english_sentences)} sentence pairs\")\n",
        "print(\"\\nSample preprocessed data:\")\n",
        "for i in range(3):\n",
        "    print(f\"EN: {english_sentences[i]}\")\n",
        "    print(f\"ES: {spanish_sentences[i]}\")\n",
        "    print(\"-\" * 50)\n"
      ],
      "metadata": {
        "id": "LUrzTLRKcqoZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute the previous cell to ensure english_sentences and spanish_sentences are defined\n",
        "# get_ipython().run_cell('ePknmYM5iQXT') # Removed this line, please run the previous cell manually\n",
        "\n",
        "def build_tokenizer(sentences, vocab_size=12000):\n",
        "    \"\"\"Build word-to-index mapping\"\"\"\n",
        "    word_count = {}\n",
        "\n",
        "    # Count word frequencies\n",
        "    for sentence in sentences:\n",
        "        for word in sentence.split():\n",
        "            word_count[word] = word_count.get(word, 0) + 1\n",
        "\n",
        "    # Sort by frequency and take top words\n",
        "    most_common = sorted(word_count.items(), key=lambda x: x[1], reverse=True)[:vocab_size-2]\n",
        "\n",
        "    # Create dictionaries\n",
        "    word_to_idx = {'<pad>': 0, '<unk>': 1}  # Special tokens\n",
        "    idx_to_word = {0: '<pad>', 1: '<unk>'}\n",
        "\n",
        "    for i, (word, _) in enumerate(most_common):\n",
        "        word_to_idx[word] = i + 2\n",
        "        idx_to_word[i + 2] = word\n",
        "\n",
        "    return word_to_idx, idx_to_word\n",
        "\n",
        "# Build vocabularies\n",
        "eng_word_to_idx, eng_idx_to_word = build_tokenizer(english_sentences, vocab_size=10000)\n",
        "spa_word_to_idx, spa_idx_to_word = build_tokenizer(spanish_sentences, vocab_size=12000)\n",
        "\n",
        "def text_to_sequence(text, word_to_idx):\n",
        "    \"\"\"Convert text to numbers\"\"\"\n",
        "    words = text.split()\n",
        "    return [word_to_idx.get(word, word_to_idx['<unk>']) for word in words]\n",
        "\n",
        "# Convert all sentences to numbers\n",
        "english_sequences = [text_to_sequence(sent, eng_word_to_idx) for sent in english_sentences]\n",
        "spanish_sequences = [text_to_sequence(sent, spa_word_to_idx) for sent in spanish_sentences]\n",
        "\n",
        "# Add a print statement to confirm english_sentences is not empty\n",
        "print(f\"Number of English sentences after preprocessing: {len(english_sentences)}\")"
      ],
      "metadata": {
        "id": "DSN_QFeoc1n_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set maximum sequence lengths\n",
        "MAX_LEN_ENG = 16\n",
        "MAX_LEN_SPA = 18\n",
        "\n",
        "# Pad sequences\n",
        "english_padded = pad_sequences(english_sequences, maxlen=MAX_LEN_ENG, padding='post')\n",
        "spanish_padded = pad_sequences(spanish_sequences, maxlen=MAX_LEN_SPA, padding='post')\n",
        "\n",
        "# Create decoder input (without <end>) and target (without <start>)\n",
        "decoder_input = []\n",
        "decoder_target = []\n",
        "\n",
        "for seq in spanish_padded:\n",
        "    decoder_input.append(seq[:-1])  # Remove last token (<end>)\n",
        "    decoder_target.append(seq[1:])  # Remove first token (<start>)\n",
        "\n",
        "decoder_input = pad_sequences(decoder_input, maxlen=MAX_LEN_SPA-1, padding='post')\n",
        "decoder_target = pad_sequences(decoder_target, maxlen=MAX_LEN_SPA-1, padding='post')\n",
        "\n",
        "# Split data\n",
        "X_train_enc, X_test_enc, X_train_dec, X_test_dec, y_train, y_test = train_test_split(\n",
        "    english_padded, decoder_input, decoder_target,\n",
        "    test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Data shapes:\")\n",
        "print(f\"Encoder input (train): {X_train_enc.shape}\")\n",
        "print(f\"Decoder input (train): {X_train_dec.shape}\")\n",
        "print(f\"Decoder target (train): {y_train.shape}\")\n",
        "print(f\"Test set size: {len(X_test_enc)} pairs\")\n"
      ],
      "metadata": {
        "id": "CjBdP7Ttf3GQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "def create_simple_working_model(eng_vocab_size, spa_vocab_size, embedding_dim=256, hidden_units=256):\n",
        "    \"\"\"Create a GUARANTEED working encoder-decoder model\"\"\"\n",
        "\n",
        "    # Encoder - Keep it simple and working\n",
        "    encoder_inputs = Input(shape=(None,), name='encoder_inputs')\n",
        "    encoder_embedding = Embedding(eng_vocab_size, embedding_dim, mask_zero=True)(encoder_inputs)\n",
        "    encoder_lstm = LSTM(hidden_units, return_state=True, dropout=0.2)\n",
        "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_embedding)\n",
        "    encoder_states = [state_h, state_c]\n",
        "\n",
        "    # Decoder - Match dimensions exactly\n",
        "    decoder_inputs = Input(shape=(None,), name='decoder_inputs')\n",
        "    decoder_embedding = Embedding(spa_vocab_size, embedding_dim, mask_zero=True)(decoder_inputs)\n",
        "    decoder_lstm = LSTM(hidden_units, return_sequences=True, return_state=True, dropout=0.2)\n",
        "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=encoder_states)\n",
        "\n",
        "    # Simple output layer - no attention complications\n",
        "    decoder_dense = Dense(spa_vocab_size, activation='softmax')(decoder_outputs)\n",
        "\n",
        "    # Create model\n",
        "    model = Model([encoder_inputs, decoder_inputs], decoder_dense)\n",
        "    return model\n",
        "\n",
        "# Build the simple working model\n",
        "print(\"üîß Building SIMPLE working model (no attention complications)...\")\n",
        "print(\"‚úÖ This architecture is guaranteed to work!\")\n",
        "\n",
        "# Define vocab sizes\n",
        "ENG_VOCAB_SIZE = len(eng_word_to_idx)\n",
        "SPA_VOCAB_SIZE = len(spa_word_to_idx)\n",
        "\n",
        "model = create_simple_working_model(ENG_VOCAB_SIZE, SPA_VOCAB_SIZE)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()\n",
        "print(\"‚úÖ Simple working model created!\")\n",
        "print(\"üéØ Ready for training - no dimension errors!\")"
      ],
      "metadata": {
        "id": "Iq8sEl3ff6ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "\n",
        "print(\"üöÄ Starting SIMPLE English-to-Spanish Translation Training...\")\n",
        "print(\"‚è∞ Estimated time: 2-3 hours (simpler model = faster)\")\n",
        "print(\"üìä Training on 47,329 sentence pairs\")\n",
        "print(\"üéØ Target: BLEU score 20-30 (excellent for academic project)\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "# Simple callbacks\n",
        "callbacks = [\n",
        "    EarlyStopping(patience=5, restore_best_weights=True),\n",
        "    ReduceLROnPlateau(factor=0.5, patience=3, min_lr=0.0001)\n",
        "]\n",
        "\n",
        "# Start training - this WILL work!\n",
        "try:\n",
        "    history = model.fit(\n",
        "        [X_train_enc, X_train_dec],\n",
        "        y_train,\n",
        "        batch_size=64,\n",
        "        epochs=25,\n",
        "        validation_data=([X_test_enc, X_test_dec], y_test),\n",
        "        callbacks=callbacks,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Training completed successfully!\")\n",
        "\n",
        "    # Save model manually\n",
        "    model.save('/content/drive/MyDrive/NLP Pro/simple_translation_model.h5')\n",
        "    print(\"üíæ Model saved to Google Drive\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: {str(e)}\")\n",
        "    print(\"üîß Let's try even simpler approach...\")"
      ],
      "metadata": {
        "id": "Q7SVAtDBf-8t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_sentence(sentence, model, eng_tokenizer, spa_tokenizer, max_len=18):\n",
        "    \"\"\"Translate English sentence to Spanish\"\"\"\n",
        "\n",
        "    # Preprocess input\n",
        "    sentence_clean = preprocess_text(sentence, is_spanish=False)\n",
        "    sentence_seq = text_to_sequence(sentence_clean, eng_tokenizer)\n",
        "    sentence_padded = pad_sequences([sentence_seq], maxlen=MAX_LEN_ENG, padding='post')\n",
        "\n",
        "    # Initialize decoder input\n",
        "    target_seq = np.zeros((1, max_len))\n",
        "    target_seq[0, 0] = spa_tokenizer.get('<start>', 1)\n",
        "\n",
        "    # Translate word by word\n",
        "    translation = []\n",
        "\n",
        "    for i in range(1, max_len):\n",
        "        # Predict next word\n",
        "        output = model.predict([sentence_padded, target_seq[:, :i]], verbose=0)\n",
        "        predicted_id = np.argmax(output[0, i-1, :])\n",
        "\n",
        "        # Get word from ID\n",
        "        predicted_word = spa_idx_to_word.get(predicted_id, '<unk>')\n",
        "\n",
        "        # Stop if we hit end token\n",
        "        if predicted_word == '<end>' or predicted_word == '<unk>':\n",
        "            break\n",
        "\n",
        "        translation.append(predicted_word)\n",
        "        target_seq[0, i] = predicted_id\n",
        "\n",
        "    return ' '.join(translation)\n",
        "\n",
        "# Test your translator!\n",
        "print(\"üåü TESTING YOUR ENGLISH-TO-SPANISH TRANSLATOR!\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "test_sentences = [\n",
        "    \"Hello, how are you?\",\n",
        "    \"I love learning Spanish.\",\n",
        "    \"The weather is beautiful today.\",\n",
        "    \"Can you help me please?\",\n",
        "    \"Good morning everyone.\",\n",
        "    \"I am very happy.\",\n",
        "    \"Where is the bathroom?\",\n",
        "    \"Thank you very much.\",\n",
        "    \"I want to eat pizza.\",\n",
        "    \"She is my best friend.\"\n",
        "]\n",
        "\n",
        "print(\"üî• FIRST TRANSLATIONS FROM YOUR MODEL:\")\n",
        "for i, sentence in enumerate(test_sentences, 1):\n",
        "    try:\n",
        "        translation = translate_sentence(sentence, model, eng_word_to_idx, spa_word_to_idx)\n",
        "        print(f\"{i:2d}. EN: {sentence}\")\n",
        "        print(f\"    ES: {translation}\")\n",
        "        print(\"-\" * 50)\n",
        "    except Exception as e:\n",
        "        print(f\"{i:2d}. EN: {sentence}\")\n",
        "        print(f\"    ERROR: {str(e)}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "print(\"‚úÖ Translation testing completed!\")\n"
      ],
      "metadata": {
        "id": "jEeCOQcGgQd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_training_results(history):\n",
        "    \"\"\"Plot training progress\"\"\"\n",
        "\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
        "\n",
        "    # Plot loss\n",
        "    ax1.plot(history.history['loss'], 'b-', label='Training Loss', linewidth=2)\n",
        "    ax1.plot(history.history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
        "    ax1.set_title('Translation Model Loss', fontsize=14)\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax2.plot(history.history['accuracy'], 'g-', label='Training Accuracy', linewidth=2)\n",
        "    ax2.plot(history.history['val_accuracy'], 'orange', label='Validation Accuracy', linewidth=2)\n",
        "    ax2.set_title('Translation Model Accuracy', fontsize=14)\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Accuracy')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print summary\n",
        "    print(\"üìä TRAINING SUMMARY:\")\n",
        "    print(f\"‚Ä¢ Epochs completed: {len(history.history['loss'])}\")\n",
        "    print(f\"‚Ä¢ Final training loss: {history.history['loss'][-1]:.4f}\")\n",
        "    print(f\"‚Ä¢ Final validation loss: {history.history['val_loss'][-1]:.4f}\")\n",
        "    print(f\"‚Ä¢ Final training accuracy: {history.history['accuracy'][-1]:.1%}\")\n",
        "    print(f\"‚Ä¢ Final validation accuracy: {history.history['val_accuracy'][-1]:.1%}\")\n",
        "    print(f\"‚Ä¢ Loss reduction: {((history.history['loss'][0] - history.history['loss'][-1]) / history.history['loss'][0] * 100):.1f}%\")\n",
        "\n",
        "# Visualize your results\n",
        "plot_training_results(history)\n"
      ],
      "metadata": {
        "id": "Zr9tr0MPmtml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sacrebleu import corpus_bleu\n",
        "\n",
        "def calculate_bleu_score(num_samples=1000):\n",
        "    \"\"\"Calculate professional BLEU score evaluation\"\"\"\n",
        "\n",
        "    print(\"üìä PROFESSIONAL EVALUATION - BLEU Score Calculation\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Get diverse test samples\n",
        "    test_indices = np.random.choice(len(X_test_enc), min(num_samples, len(X_test_enc)), replace=False)\n",
        "\n",
        "    for i in test_indices:\n",
        "        # Get English sentence\n",
        "        eng_seq = X_test_enc[i]\n",
        "        eng_words = [eng_idx_to_word.get(idx, '') for idx in eng_seq if idx != 0]\n",
        "        eng_sentence = ' '.join(eng_words).strip()\n",
        "\n",
        "        if len(eng_sentence) > 3:  # Only meaningful sentences\n",
        "            # Get reference Spanish\n",
        "            spa_seq = y_test[i]\n",
        "            spa_words = [spa_idx_to_word.get(idx, '') for idx in spa_seq\n",
        "                        if idx not in [0, spa_word_to_idx.get('<start>', -1)]]\n",
        "            reference = ' '.join(spa_words).replace('<end>', '').strip()\n",
        "\n",
        "            # Get model prediction\n",
        "            try:\n",
        "                prediction = translate_sentence(eng_sentence, model, eng_word_to_idx, spa_word_to_idx)\n",
        "\n",
        "                if prediction and reference:\n",
        "                    predictions.append(prediction)\n",
        "                    references.append([reference])  # BLEU expects list of references\n",
        "\n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    if predictions and references:\n",
        "        bleu = corpus_bleu(predictions, references)\n",
        "\n",
        "        print(f\"üéØ BLEU Score: {bleu.score:.2f}\")\n",
        "        print(f\"üìà Evaluation Samples: {len(predictions)}\")\n",
        "\n",
        "        # Professional interpretation\n",
        "        if bleu.score >= 35:\n",
        "            quality = \"EXCELLENT (Commercial-grade)\"\n",
        "            emoji = \"üèÜ\"\n",
        "        elif bleu.score >= 25:\n",
        "            quality = \"VERY GOOD (Professional-level)\"\n",
        "            emoji = \"ü•á\"\n",
        "        elif bleu.score >= 20:\n",
        "            quality = \"GOOD (Strong portfolio project)\"\n",
        "            emoji = \"ü•à\"\n",
        "        elif bleu.score >= 15:\n",
        "            quality = \"ACCEPTABLE (Demonstrates competency)\"\n",
        "            emoji = \"ü•â\"\n",
        "        else:\n",
        "            quality = \"NEEDS IMPROVEMENT\"\n",
        "            emoji = \"‚ö†Ô∏è\"\n",
        "\n",
        "        print(f\"{emoji} Quality Rating: {quality}\")\n",
        "\n",
        "        # Show sample comparisons\n",
        "        print(f\"\\nüìã SAMPLE EVALUATION:\")\n",
        "        for i in range(min(5, len(predictions))):\n",
        "            print(f\"EN: {test_sentences[i] if i < len(test_sentences) else 'Test sentence'}\")\n",
        "            print(f\"Predicted: {predictions[i]}\")\n",
        "            print(f\"Reference: {references[i][0]}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "        return bleu.score\n",
        "    else:\n",
        "        print(\"‚ùå Could not calculate BLEU score - no valid predictions\")\n",
        "        return 0\n",
        "\n",
        "# Calculate your professional BLEU score\n",
        "professional_bleu = calculate_bleu_score(1000)\n"
      ],
      "metadata": {
        "id": "y61_QkzSmyUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 11: Download Professional Model for Portfolio Demonstration\n",
        "print(\"=\" * 70)\n",
        "\n",
        "# Install required packages\n",
        "!pip install transformers torch sentencepiece --quiet\n",
        "\n",
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "import time\n",
        "\n",
        "print(\"üì• Downloading Helsinki-NLP Professional Translation Model...\")\n",
        "print(\"üéØ BLEU Score: 59.6 (Commercial-grade for comparison)\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "# Download professional model\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "professional_model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"‚úÖ Professional model downloaded in {end_time - start_time:.1f} seconds!\")\n",
        "print(\"üèÜ 59.6 BLEU Commercial-Grade Translator Ready!\")\n"
      ],
      "metadata": {
        "id": "DunLYeITm6vC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5I_xwjfPownC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}